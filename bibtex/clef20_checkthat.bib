@inproceedings{pogorelov2020fakenews,
  title={{FakeNews}: Corona Virus and {5G} Conspiracy Task at {MediaEval} 2020},
  author={Pogorelov, Konstantin and Schroeder, Daniel Thilo and Burchard, Luk and Moe, Johannes and Brenner, Stefan and Filkukova, Petra and Langguth, Johannes},
  booktitle={Proceedings of the MediaEval 2020 Workshop},
  series = {MediaEval~'20},
  year={2020}
}
@article{thorne2018fever,
  title={FEVER: a large-scale dataset for fact extraction and verification},
  author={Thorne, James and Vlachos, Andreas and Christodoulopoulos, Christos and Mittal, Arpit},
  journal={arXiv preprint arXiv:1803.05355},
  year={2018}
}
@inproceedings{thorne-etal-2018-fever,
    title = "{FEVER}: a Large-scale Dataset for Fact Extraction and {VER}ification",
    author = "Thorne, James  and
      Vlachos, Andreas  and
      Christodoulopoulos, Christos  and
      Mittal, Arpit",
    booktitle = "Proceedings of the Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    year = "2018",
	series = {NAACL-HLT~'18},
    pages = "809--819",
}
@inproceedings{da2020semeval,
  title={{SemEval}-2020 task 11: Detection of propaganda techniques in news articles},
  author={Da San Martino, Giovanni and Barr{\'o}n-Cedeno, Alberto and Wachsmuth, Henning and Petrov, Rostislav and Nakov, Preslav},
  booktitle={Proceedings of the 14th Workshop on Semantic Evaluation},
  series = {SemEval~'20},
  pages={1377--1414},
  year={2020}
}
@article{zampieri2019semeval,
title = "{S}em{E}val-2019 Task 6: Identifying and Categorizing Offensive Language in Social Media ({O}ffens{E}val)",
    author = "Zampieri, Marcos  and
      Malmasi, Shervin  and
      Nakov, Preslav  and
      Rosenthal, Sara  and
      Farra, Noura  and
      Kumar, Ritesh",
    booktitle = "Proceedings of the 13th International Workshop on Semantic Evaluation",
    series = {SemEval~'19},
    NOmonth = jun,
    year = "2019",
    address = "Minneapolis, Minnesota, USA",
    NOpublisher = "Association for Computational Linguistics",
    NOurl = "https://www.aclweb.org/anthology/S19-2010",
    NOdoi = "10.18653/v1/S19-2010",
    pages = "75--86",
    abstract = "We present the results and the main findings of SemEval-2019 Task 6 on Identifying and Categorizing Offensive Language in Social Media (OffensEval). The task was based on a new dataset, the Offensive Language Identification Dataset (OLID), which contains over 14,000 English tweets, and it featured three sub-tasks. In sub-task A, systems were asked to discriminate between offensive and non-offensive posts. In sub-task B, systems had to identify the type of offensive content in the post. Finally, in sub-task C, systems had to detect the target of the offensive posts. OffensEval attracted a large number of participants and it was one of the most popular tasks in SemEval-2019. In total, nearly 800 teams signed up to participate in the task and 115 of them submitted results, which are presented and analyzed in this report.",
}

@article{zampieri2020semeval,
  title={Semeval-2020 task 12: Multilingual offensive language identification in social media (offenseval 2020)},
  author={Zampieri, Marcos and Nakov, Preslav and Rosenthal, Sara and Atanasova, Pepa and Karadzhov, Georgi and Mubarak, Hamdy and Derczynski, Leon and Pitenis, Zeses and {\c{C}}{\"o}ltekin, {\c{C}}a{\u{g}}r{\i}},
  journal={arXiv preprint arXiv:2006.07235},
  year={2020}
}

@inproceedings{mohammad2016semeval,
  title={{SemEval}-2016 task 6: Detecting stance in tweets},
  author={Mohammad, Saif and Kiritchenko, Svetlana and Sobhani, Parinaz and Zhu, Xiaodan and Cherry, Colin},
  booktitle={Proceedings of the 10th International Workshop on Semantic Evaluation},
  pages={31--41},
  series = {SemEval~'16},
  year={2016}
}

@inproceedings{mihaylova2019semeval,
    title = "{S}em{E}val-2019 Task 8: Fact Checking in Community Question Answering Forums",
    author = "Mihaylova, Tsvetomila  and
      Karadzhov, Georgi  and
      Atanasova, Pepa  and
      Baly, Ramy  and
      Mohtarami, Mitra  and
      Nakov, Preslav",
    booktitle = "Proceedings of the 13th International Workshop on Semantic Evaluation",
    series = {SemEval~'19},
    NOmonth = jun,
    year = "2019",
    NOaddress = "Minneapolis, Minnesota, USA",
    NOpublisher = "Association for Computational Linguistics",
    NOurl = "https://www.aclweb.org/anthology/S19-2149",
    NOdoi = "10.18653/v1/S19-2149",
    pages = "860--869",
    abstract = "We present SemEval-2019 Task 8 on Fact Checking in Community Question Answering Forums, which features two subtasks. Subtask A is about deciding whether a question asks for factual information vs. an opinion/advice vs. just socializing. Subtask B asks to predict whether an answer to a factual question is true, false or not a proper answer. We received 17 official submissions for subtask A and 11 official submissions for Subtask B. For subtask A, all systems improved over the majority class baseline. For Subtask B, all systems were below a majority class baseline, but several systems were very close to it. The leaderboard and the data from the competition can be found at http://competitions.codalab.org/competitions/20022.",
}

@inproceedings{mihaylova-etal-2019-semeval,
    title = "{S}em{E}val-2019 Task 8: Fact Checking in Community Question Answering Forums",
    author = "Mihaylova, Tsvetomila  and
      Karadzhov, Georgi  and
      Atanasova, Pepa  and
      Baly, Ramy  and
      Mohtarami, Mitra  and
      Nakov, Preslav",
    booktitle = "Proceedings of the 13th International Workshop on Semantic Evaluation",
    year = "2019",
    pages = "860--869",
    series = {SemEval~'19},
}

@inproceedings{gorrell2019semeval,
  title={{SemEval}-2019 task 7: {RumourEval}, determining rumour veracity and support for rumours},
  author={Gorrell, Genevieve and Kochkina, Elena and Liakata, Maria and Aker, Ahmet and Zubiaga, Arkaitz and Bontcheva, Kalina and Derczynski, Leon},
  booktitle={Proceedings of the 13th International Workshop on Semantic Evaluation},
  series = {SemEval~'19},
  pages={845--854},
  year={2019}
}

@inproceedings{derczynski2017semeval,
    title = "{S}em{E}val-2017 Task 8: {R}umour{E}val: Determining rumour veracity and support for rumours",
    author = "Derczynski, Leon  and
      Bontcheva, Kalina  and
      Liakata, Maria  and
      Procter, Rob  and
      Wong Sak Hoi, Geraldine  and
      Zubiaga, Arkaitz",
    booktitle = "Proceedings of the 11th International Workshop on Semantic Evaluation",
    series = {SemEval~'17},
    NOmonth = aug,
    year = "2017",
    NOaddress = "Vancouver, Canada",
    NOpublisher = "Association for Computational Linguistics",
    NOurl = "https://www.aclweb.org/anthology/S17-2006",
    NOdoi = "10.18653/v1/S17-2006",
    pages = "69--76",
    abstract = "Media is full of false claims. Even Oxford Dictionaries named {``}post-truth{''} as the word of 2016. This makes it more important than ever to build systems that can identify the veracity of a story, and the nature of the discourse around it. RumourEval is a SemEval shared task that aims to identify and handle rumours and reactions to them, in text. We present an annotation scheme, a large dataset covering multiple topics {--} each having their own families of claims and replies {--} and use these to pose two concrete challenges as well as the results achieved by participants on these challenges.",
}

@InProceedings{clef-checkthat-lncs:2020,
 author = "Barr\'{o}n-Cede{\~n}o, Alberto and
    Elsayed, Tamer and
    Nakov, Preslav and
    {Da San Martino}, Giovanni and 
    Hasanain, Maram and    
    Suwaileh, Reem and
    Haouari, Fatima and
    Babulkov, Nikolay and
    Hamdan, Bayan and 
    Nikolov, Alex and    
    Shaar, Shaden and
    {Sheikh Ali}, Zien",
 title  = "{Overview of CheckThat! 2020}: Automatic Identification and
Verification of Claims in Social Media",
 pages="215--236",
 crossref = "CLEF_LNCS:20",
}


@InProceedings{clef-checkthat-ar:2020,
 author = "Hasanain, Maram and    
    Haouari, Fatima and
    Suwaileh, Reem and
    Ali, {Zien Sheikh} and
    Hamdan, Bayan and 
    Elsayed, Tamer and
    Barr\'{o}n-Cede{\~n}o, Alberto and
    {Da San Martino}, Giovanni and
    Nakov, Preslav",
 title = "Overview of {CheckThat!} 2020 {A}rabic: Automatic Identification and Verification of Claims in Social Media",
 crossref = "clef2020-workingnotes"
}

@InProceedings{clef-checkthat-en:2020,
 author = "Shaar, Shaden and
    Nikolov, Alex and
    Babulkov, Nikolay and
    Alam, Firoj and  
    Barr\'{o}n-Cede{\~n}o, Alberto and
    Elsayed, Tamer and
    Hasanain, Maram and    
    Suwaileh, Reem and
    Haouari, Fatima and
    {Da San Martino}, Giovanni and
    Nakov, Preslav",
 title = "Overview of {CheckThat!} 2020 {E}nglish: Automatic Identification and Verification of Claims in Social Media",
 crossref = "clef2020-workingnotes"
}

@InProceedings{clef-checkthat-Alkhalifa:2020,
 author = "Alkhalifa, Rabab and
		Yoong, Theodore and
		Kochkina, Elena and
		Zubiaga, Arkaitz and
		Liakata, Maria",
 title = "{QMUL-SDS} at {CheckThat!} 2020: {D}etermining {COVID-19} Tweet 
Check-Worthiness Using an Enhanced {CT-BERT} with Numeric Expressions",
 abstract = "This paper describes the QMUL-SDS team's entry for Task 1 of the 
CLEF 2020 CheckThat! shared task, where we determined the check-worthiness of 
tweets about COVID-19 to prioritise tweets for fact-checking. We utilised a BERT 
language model with binary classification and sigmoid for ranking. In 
particular, we used the uncased COVID-Twitter-BERT architecture that had been 
pre-trained on COVID-19 Twitter stream data using two different language 
modelling tasks: whole word-masked modelling and next sentence prediction. The 
model was trained by jointly optimising a binary cross-entropy loss. We 
performed traditional Twitter data preprocessing in order to improve system 
performance: (1) All Twitter account handles, digits, hashtags and hyperlinks 
were removed; (2) A vector look-up was then used for each token from CO-BERT 
model; (3) Each token was thereafter mapped to a unique integer in the corpus’ 
vocabulary, done using two tokenisers which performed differently in different 
cases; (4) Padding was used to equalise the lengths of the input sequences in a 
batch, i.e. we increase the length of some sequences by adding tokens. We 
attempted to reduce the padding by letting our model decide the padding length 
based on the given batch and its longest sequence, reducing any unnecessary 
overhead carried by the model. We ranked fourth in the task.",
 crossref = "clef2020-workingnotes"
}



@InProceedings{clef-checkthat-Bouziane:2020,
 author = "Bouziane, Mostafa and
        Perrin, Hugo and
        Cluzeau, Aur\`elien and
        Mardas, Julien and
        Sadeq, Amine",
 title = "{Buster.AI} at {CheckThat!} 2020: {I}nsights and recommendations to improve Fact-Checking.",
 abstract = "As part of the CheckThat! 2020 Task 2, we investigated sentence similarity using transformer models. In Task 2, the goal was to effectively rank claims based on their relevancy compared to an input tweet. While setting our baseline on sentence similarity for Fact-Checking, we gathered insights we felt compelled to share in this paper. We learned how multimodal data utilization could foster significant uplifts in model performance. We also gained knowledge on which hybrid training and strong sampling worked best for Fact-Checking applications, and wanted to share our interpretation of the results we got. Finally, we want to explain our recommendations on data augmentations. All of the above allowed us to set our baseline in Fact-Checking in the CLEF Checkthat! 2020 Task 2 competition.",
 crossref = "clef2020-workingnotes"
}

@InProceedings{clef-checkthat-cheema:2020,
 author = "Cheema, Gullal S. and
		Hakimov, Sherzod and
		Ewerth, Ralph",
 title = "Check\_square at {CheckThat!} 2020: {C}laim Detection in Social Media via Fusion of Transformer and Syntactic Features",
 abstract = "In this paper, we focus on solving two problems in the fact-check ecosystem, claim check-worthiness prediction and claim retrieval. For the first problem, we explore the fusion of syntactic features and deep transformer Bidirectional Encoder Representations from Transformers (BERT) embeddings, to classify check-worthiness of a tweet, whether it includes a claim or not. We use Part-of-speech (POS) tags, named entities, and dependency relations as syntactic features and a combination of hidden layers in BERT to compute tweet embedding, and concatenate them to get a large feature embedding. We then reduce the feature size by applying PCA and train an SVM classifier for binary classification of tweets. We conduct a detailed analysis of syntactic feature combinations, word2vec embeddings and BERT hidden layer combinations to find the best performing model. For the second problem, we explore the pre-trained embeddings from a Siamese network transformer model (sentence transformers) specifically trained for semantic textual similarity and perform KD-search to retrieve claims. We further fine-tune these models using triplets with a triplet loss and conduct detailed experiments with different backbone networks.",
 crossref = "clef2020-workingnotes"
}


@InProceedings{clef-checkthat-Kartal:2020,
 author = "Kartal, Yavuz Selim and
		Kutlu, Mucahid",
 title = "{TOBB ETU at CheckThat!} 2020: Prioritizing {E}nglish and {A}rabic Claims Based on Check-Worthiness",
 abstract = "Today’s one of the most popular information resource is the social media and any information as well as misinformation can reach out to thousands of users in a short time. Therefore, automated systems that can prioritize claims based on their check-worthiness have a big role to prevent spreading of misinformation. In this paper, we propose three different models for CLEF Check That! Lab. In all our models, we use logistic regression. The first one uses BERT and word embeddings together. The second model is a hybrid combination of BERT and the first model such that we use only fine-tuned BERT for the first 15 claims. For the others, we use the first model. The last model is a modification of the first model with additional features which are pos tags, domain-specific controversial topics and a handcrafted word list. For the Arabic task, we use AraBert as our Bert model.  We use the first and second models for Task 1 Arabic and all models for Task 1 English and Task 5.
In the official evaluation of primary submissions, our primary models a) ranked 3rd in Task 1 Arabic based on P@30 and shared the 1st rank with another group on P@5,  b) ranked 4th in Task 1 English based on MAP and shared the 1st rank with 5 other groups on RR, P@1, P@3 and P@5, and c) ranked 3rd in Task 5 based on MAP.",
 crossref = "clef2020-workingnotes"
}

@InProceedings{clef-checkthat-Martinez-Rico:2020,
 author = "Martinez-Rico, {Juan R.} and
		Araujo, Lourdes and
		Martinez-Romo, Juan",
 title = "{NLP\&IR@UNED at CheckThat!} 2020: {A} Preliminary Approach for 
Check-Worthiness and Claim Retrieval Tasks using Neural Networks and Graphs",
 abstract = "Check-Worthiness and Claim Retrieval are two of the first tasks to 
be performed on the Fake News detection pipeline. In this article we present our 
approach to these tasks presented in the 2020 edition of the CheckThat! Labs. In 
the Task 1, Tweet Check-Worthiness English, we propose a Bi-LSTM model with 
Glove Twitter embeddings where the number of inputs has been increased with a 
graph generated from the additional information provided for each tweet. In Task 
1 Arabic we have followed a similar approach but using a FFNN model with Arabic 
embeddings. For the task 5, Debate Check-Worthiness, we propose a naive Bi-LSTM 
model with Glove embeddings. Finally our aproach to the Task 2, Claim Retrieval, 
is based in a FFNN model with features such as cosine similarity over USE 
embeddings of tweet and claim, and other linguistic features extracted from both 
elements.",
 crossref = "clef2020-workingnotes"
}

@InProceedings{clef-checkthat-McDonald:2020,
 author = "McDonald, Thomas and
		Dong, ZiQing and
		Zhang, Yingji and
		Hampson, Rebekah and
		Young, James and
		Cao, Qianyu and 
		Leidner, Jochen and
        Stevenson, Mark",
 title = "The {U}niversity of {S}heffield at {CheckThat!} 2020: {C}laim 
		Identification and Verification on {T}witter",
 abstract = "The spread of misinformation online has been gathering pace in  
recent years, and whilst claim verification is an active area of research, the 
COVID-19 pandemic has presented a unique challenge due to the large amount of 
inaccurate information being shared on social media platforms. This paper 
describes our entries to Tasks 1 and 2 of the CLEF 2020 CheckThat! lab, which 
focus on the problems of determining check-worthiness and verification of claims 
found in tweets related to COVID-19. Our best performing approach for Task 1 
employed TF-IDF term weightings and a Random Forest model, ranking 17th out of 
27 systems. For Task 2, our best system involved the use of TF-IDF term 
weightings with a BM25 similarity score and a Support Vector Classifier scoring 
model, ranking 13th out of 22 systems.",
 crossref = "clef2020-workingnotes"
}

@InProceedings{clef-checkthat-Nikolov:2020,
 author = "Nikolov, Alex and {Da San Martino}, Giovanni and Koychev, Ivan and Nakov, Preslav",
 title = "{Team\_Alex at CheckThat!} 2020: Identifying check-worthy tweets with transformer models",
 crossref = "clef2020-workingnotes"
}


@InProceedings{clef-checkthat-Passaro:2020,
 author = "Passaro, {Lucia C.} and
		Bondielli, Alessandro and
		Lenci, Alessandro and
		Marcelloni, Francesco",
title = "{UNIPI-NLE} at {CheckThat!} 2020: {A}pproaching Fact Checking from a  
Sentence Similarity Perspective Through the Lens of Transformers",
abstract = "This paper describes a Fact Checking system based on a combination 
of Information Extraction and Deep Learning strategies to associate a tweet 
with the corresponding claim. The system has been built starting from a 
pre-trained Sentence-BERT model on which two cascade fine-tuning steps have been 
applied. The first step is a sentence similarity task aimed at predicting the 
cosine similarity between a tweet and a claim. Sentence-BERT was used to fine 
tune the model on this task by building a training set in which positive 
examples, consisting of gold tweet-claim pairs, were associated with a maximum 
cosine similarity. Negative examples were instead randomly selected from a list 
of candidate pairs with a non empty overlap in terms of metadata (named entities 
and keywords). Such examples were associated with the original cosine similarity 
between the tweet and claim, lowered by applying a penalty function. The second 
step consists in a binary classification task aimed at deciding whether a 
tweet-claim pair is a correct match or not. The model fine-tuned in the first 
step was enriched with a classifier head on top. Positive and negative examples 
were selected as above to construct the training set. The final ranking produced 
by the system is the predicted probability of the pair labelled as correct. 
Overall, the system reached a 0.91 MAP@5 on the test set.",
crossref = "clef2020-workingnotes"
}


@InProceedings{clef-checkthat-Thuma:2020,
 author   = "Thuma, Edwin and
		Motlogelwa, Nkwebi Peace and
		Leburu-Dingalo, Tebo and
		Mudongo, Monkgogi",
 title    = "{UB\_ET} at {CheckThat!} 2020: {E}xploring Ad hoc Retrieval Approaches in Verified Claims Retrieval",
 abstract = "In this paper, we explore three different ad hoc retrieval approaches to rank verified claims, 
 so that those that verify the input claim are ranked on top. In particular, we deploy DPH Divergence from Randomness (DFR) term weighting model to rank the verified claims.
 In addition, we deploy the Sequential Dependence (SD) variant of the Markov Random Fields (MRF) for term dependence to re-rank documents (verified claims) that have query terms (input claim) in close proximity.
 Moreover, we deploy LambdaMART, which is a learning to rank algorithm that use machine learning techniques to learn an appropriate combination of features into an effective 
ranking model.",
 crossref = "clef2020-workingnotes"
}

@InProceedings{clef-checkthat-Hasanain:2020,
 author = "Hasanain, Maram and
		Elsayed, Tamer",
 title = "big{IR} at {CheckThat!} 2020: {M}ultilingual {BERT} for Ranking {A}rabic Tweets by Check-worthiness",
 abstract = "In this work we use Multilingual bERT pre-trained language model for ranking tweets by check-worthiness.",
 crossref = "clef2020-workingnotes"
}


@InProceedings{clef-checkthat-Touahri:2020,
 author   = "Touahri, Ibtissam and
		Mazroui, Azzeddine",
 title    = "{EvolutionTeam} at {CheckThat!} 2020: {I}ntegration of linguistic and sentimental features in a fake news detection approach",
 crossref = "clef2020-workingnotes"
}

@InProceedings{clef-checkthat-Hussein:2020,
 author = "Hussein, Ahmad and
		Hussein, Abdulkarim and
		Ghneim, Nada and
		Joukhadar, Ammar",
 title = "{DamascusTeam} at {CheckThat!} 2020: {C}heck Worthiness on {T}witter with Hybrid {CNN} and {RNN} Models",
 abstract = "In recent years, online social networks like Twitter, Facebook, Instagram, and others have revolutionized interpersonal communication and it becomes an important platform to share information about current events. Consequently, the research on the informativeness of posts is becoming more important than ever before. In this paper, we present our approach to analyze the informativeness of Arabic information on Twitter. We used a hybrid system of convolutional neural networks and long-short term recurrent neural network models. To train the classification model, in addition to the dataset of 1500 tweets provided from CLEF 2020, we annotated for a data set of 5000 Arabic tweets -corresponding to 4 high impact news events of 2020 around the world. Results show that we can extract the informativeness of tweets with high effectiveness.",
 crossref = "clef2020-workingnotes"
}

@InProceedings{clef-checkthat-williams:2020,
 author = "Williams, Evan and
		Rodrigues, Paul and
		Novak, Valerie",
 title = "Accenture at {CheckThat! 2020: I}f you say so: Post-hoc fact-checking of claims using transformer-based models",
 abstract = "The Accenture team will describe experiments in using transformer fine-tuning to automatically fact-check tweets in English and Arabic.
We fine-tuned the English training data with the RoBERTa base model which was created and open-sourced by Liu et al \cite{roberta}. The base-roberta model was pretrained on 160GB of text extracted from BookCorpus, English Wikipedia, CC-News, OpenWebText, and Stories. To help prevent overfitting, we added an extra mean pooling and dropout layer to the model.
We fine-tuned the Arabic training data with 3 different pre-trained Arabic BERT models- two of which were not trained on stemmed tokens. We used two BERT bases which were pretrained and made open source by Wissam Antoun, Fady Baly, and Hazem Hajj at the American University of Beiruit \cite{aubmindlabs}. The Aubmindlabs Arabert was pretrained on Arabic Articles manually scraped by researchers along with two open source corpora: (1) the 1.5 billion word Arabic Corpus, which contains more than 5 million news articles from ten major news sources. and (2) OSIAN: the Open Source International Arabic News Corpus, which includes more than 3.5 million articles from 31  news sources \cite{aubmindlabs}
In order to improve label balance in the Arabic corpus, and to improve lexical diversity, we used machine translation as an external data source to upsample the positive class of the Arabic training data.  We took the Arabic text, and used Amazon Translate to translate this into English and then back to Arabic.  These translated tweets served as additional training data.
While metadata was provided for the Arabic and English tweets, this approach utilized only the language content.",
 crossref = "clef2020-workingnotes"
}
%%%%%%%%%%%
% PROCEEDINGS
%%%%%%%%%%%

@proceedings{CLEF_LNCS:20,
 editor =  "Arampatzis, Avi and
    Kanoulas, Evangelos and 
    Tsikrika, Theodora and 
    Vrochidis, Stefanos and 
    Joho, Hideo and 
    Lioma, Christina and 
    Eickhoff, Carsten and
    Névéol, Aurélie and 
    Cappellato, Linda and 
    Ferro, Nicola ",
 booktitle = "{Experimental {IR} Meets Multilinguality, Multimodality, and Interaction. Proceedings of the Eleventh International Conference of the {CLEF} Association 
({CLEF} 2020)}",
 series    = "LNCS (12260)",
 publisher = "Springer",
 year = 2020 
}


@proceedings{clef2020-workingnotes,
 editor = "Cappellato, Linda 
    and Eickhoff, Carsten 
    and Ferro, Nicola and
    Névéol, Aurélie",
    title = "CLEF 2020 Working Notes",
    series    = "{CEUR} Workshop Proceedings",
    NOpublisher = "CEUR-WS.org",
    issn= "1613-0073",
    year = 2020,
    NOaddress = {Thessaloniki, Greece},
}

@inproceedings{antoun2020arabert,
    title = "{A}ra{BERT}: Transformer-based Model for {A}rabic Language Understanding",
    author = "Antoun, Wissam  and
      Baly, Fady  and
      Hajj, Hazem",
    booktitle = "Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools",
    series = {OSAC~'20},
    NOmonth = may,
    year = "2020",
    address = "Marseille, France",
    NOpublisher = "European Language Resource Association",
    NOurl = "https://www.aclweb.org/anthology/2020.osact-1.2",
    pages = "9--15",
    abstract = "The Arabic language is a morphologically rich language with relatively few resources and a less explored syntax compared to English. Given these limitations, Arabic Natural Language Processing (NLP) tasks like Sentiment Analysis (SA), Named Entity Recognition (NER), and Question Answering (QA), have proven to be very challenging to tackle. Recently, with the surge of transformers based models, language-specific BERT based models have proven to be very efficient at language understanding, provided they are pre-trained on a very large corpus. Such models were able to set new standards and achieve state-of-the-art results for most NLP tasks. In this paper, we pre-trained BERT specifically for the Arabic language in the pursuit of achieving the same success that BERT did for the English language. The performance of AraBERT is compared to multilingual BERT from Google and other state-of-the-art approaches. The results showed that the newly developed AraBERT achieved state-of-the-art performance on most tested Arabic NLP tasks. The pretrained araBERT models are publicly available on https://github.com/aub-mind/araBERT hoping to encourage research and applications for Arabic NLP.",
    language = "English",
    ISBN = "979-10-95546-51-1",
}
@inproceedings{devlin2019bert,
  title={{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  series = {NAACL-HLT~'19},
  address = {Minneapolis, Minnesota, USA},
  pages={4171--4186},
  year={2019}
 }


@inproceedings{shahifakecovid,
  title={Fake{C}ovid -- A Multilingual Cross-domain Fact Check News Dataset for {COVID-19}},
  author={Shahi, Gautam Kishore and Nandini, Durgesh},
  booktitle={Workshop Proceedings of the 14th International {AAAI} {C}onference on {W}eb and {S}ocial {M}edia},
  year = {2020},
  Ourl = {http://workshop-proceedings.icwsm.org/pdf/2020_14.pdf}
}


@article{shahi2020exploratory,
  title={An Exploratory Study of {COVID-19} Misinformation on {Twitter}},
  author={Shahi, Gautam Kishore and Dirkson, Anne and Majchrzak, Tim A},
  journal={arXiv preprint arXiv:2005.05710},
  year={2020}
}

@article{shahi2020amused,
  title={{AMUSED}: An Annotation Framework of Multi-modal Social Media Data},
  author={Shahi, Gautam Kishore},
  journal={arXiv:2010.00502},
  year={2020}
}

@article{oshikawa2018survey,
  title={A survey on natural language processing for fake news detection},
  author={Oshikawa, Ray and Qian, Jing and Wang, William Yang},
  journal={arXiv preprint arXiv:1811.00770},
  year={2018}
}

@inproceedings{oshikawa-etal-2020-survey,
    title = "A Survey on Natural Language Processing for Fake News Detection",
    author = "Oshikawa, Ray  and
      Qian, Jing  and
      Wang, William Yang",
    booktitle = "Proceedings of the 12th Language Resources and Evaluation Conference",
    year = "2020",
    series = {LREC~'20},
    pages = "6086--6093",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@article{li2012truth,
 author = {Li, Xian and Dong, Xin Luna and Lyons, Kenneth and Meng, Weiyi and Srivastava, Divesh},
 year = {2012},
 title = {Truth finding on the deep web},
 pages = {97--108},
 volume = {6},
 number = {2},
 issn = {2150-8097},
 journal = {Proceedings of the VLDB Endowment},
 doi = {10.14778/2535568.2448943}
}

@inproceedings{li2011t,
 author = {Li, Xian and Meng, Weiyi and Yu, Clement},
 title = {T-verifier: Verifying truthfulness of fact statements},
 pages = {63--74},
 publisher = {IEEE},
 isbn = {978-1-4244-8959-6},
 booktitle = {Proceedings of the 2011 IEEE 27th International Conference on Data Engineering},
 year = {2011},
 address = {Piscataway, NJ}
}

@article{li2016survey,
  title={A survey on truth discovery},
  author={Li, Yaliang and Gao, Jing and Meng, Chuishi and Li, Qi and Su, Lu and Zhao, Bo and Fan, Wei and Han, Jiawei},
  journal={ACM Sigkdd Explorations Newsletter},
  volume={17},
  number={2},
  pages={1--16},
  year={2016},
  publisher={ACM New York, NY, USA}
}

@InProceedings{CheckThat:ECIR:2020,
author="Barr{\'o}n-Cede{\~{n}}o, Alberto
and Elsayed, Tamer
and Nakov, Preslav
and Da San Martino, Giovanni
and Hasanain, Maram
and Suwaileh, Reem
and Haouari, Fatima",
NOeditor="Jose, Joemon M.
and Yilmaz, Emine
and Magalh{\~a}es, Jo{\~a}o
and Castells, Pablo
and Ferro, Nicola
and Silva, M{\'a}rio J.
and Martins, Fl{\'a}vio",
title="{CheckThat! at CLEF 2020}: Enabling the Automatic Identification and Verification of Claims in Social Media",
booktitle="Advances in Information Retrieval",
series = {ECIR~'20},
year="2020",
NOpublisher="Springer International Publishing",
NOaddress="Cham",
pages="499--507",
abstract="We describe the third edition of the CheckThat! Lab, which is part of the 2020 Cross-Language Evaluation Forum (CLEF). CheckThat! proposes four complementary tasks and a related task from previous lab editions, offered in English, Arabic, and Spanish. Task 1 asks to predict which tweets in a Twitter stream are worth fact-checking. Task 2 asks to determine whether a claim posted in a tweet can be verified using a set of previously fact-checked claims. Task 3 asks to retrieve text snippets from a given set of Web pages that would be useful for verifying a target tweet's claim. Task 4 asks to predict the veracity of a target tweet's claim using a set of potentially-relevant Web pages. Finally, the lab offers a fifth task that asks to predict the check-worthiness of the claims made in English political debates and speeches. CheckThat! features a full evaluation framework. The evaluation is carried out using mean average precision or precision at rank k for ranking tasks, and F{\$}{\$}{\_}1{\$}{\$} for classification tasks.",
isbn="978-3-030-45442-5"
}
