{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfnQvGfvtH1w"
   },
   "source": [
    "# Final Implementation of the neural re-ranking approach for CheckThat Lab Subtask 4b (Scientific Claim Source Retrieval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpDCfBMouNAL"
   },
   "source": [
    "# 1) Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rQPqDKP_QHFM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "PATH_COLLECTION_DATA = 'subtask4b_collection_data.pkl'\n",
    "df_collection = pd.read_pickle(PATH_COLLECTION_DATA)\n",
    "PATH_QUERY_TRAIN_DATA = 'subtask4b_query_tweets_train.tsv'\n",
    "PATH_QUERY_DEV_DATA = 'subtask4b_query_tweets_dev.tsv'\n",
    "df_query_train = pd.read_csv(PATH_QUERY_TRAIN_DATA, sep = '\\t')\n",
    "df_query_dev = pd.read_csv(PATH_QUERY_DEV_DATA, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Exploring optimal parameters for neural re-ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jakob\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Creating BM25 index...\n",
      "Initializing optimized reranker...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jakob\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating document embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 242/242 [11:14<00:00,  2.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created embeddings for 7718 documents\n",
      "\n",
      "=== Alpha Parameter Optimization ===\n",
      "Testing alpha=0.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:20<00:00,  9.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha=0.0 results: {1: 0.435, 5: 0.5126666666666666, 10: 0.5126666666666666}\n",
      "Testing alpha=0.1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:17<00:00, 11.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha=0.1 results: {1: 0.515, 5: 0.5676666666666667, 10: 0.5676666666666667}\n",
      "Testing alpha=0.2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:18<00:00, 10.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha=0.2 results: {1: 0.54, 5: 0.5834999999999999, 10: 0.5834999999999999}\n",
      "Testing alpha=0.3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:17<00:00, 11.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha=0.3 results: {1: 0.56, 5: 0.5989166666666667, 10: 0.5989166666666667}\n",
      "Testing alpha=0.4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:19<00:00, 10.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha=0.4 results: {1: 0.545, 5: 0.5943333333333334, 10: 0.5943333333333334}\n",
      "Testing alpha=0.5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:20<00:00,  9.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha=0.5 results: {1: 0.555, 5: 0.5989166666666667, 10: 0.5989166666666667}\n",
      "Testing alpha=0.6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:20<00:00,  9.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha=0.6 results: {1: 0.55, 5: 0.5920833333333333, 10: 0.5920833333333333}\n",
      "\n",
      "Best alpha: 0.3 with MRR@5: 0.5989166666666667\n",
      "\n",
      "=== Pool Size Optimization ===\n",
      "\n",
      "Testing candidate pool size=20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:18<00:00, 10.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pool size=20 results: {1: 0.56, 5: 0.5989166666666667, 10: 0.5989166666666667}\n",
      "\n",
      "Testing candidate pool size=30...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:19<00:00, 10.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pool size=30 results: {1: 0.56, 5: 0.6011666666666666, 10: 0.6011666666666666}\n",
      "\n",
      "Testing candidate pool size=50...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:18<00:00, 11.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pool size=50 results: {1: 0.575, 5: 0.6186666666666667, 10: 0.6186666666666667}\n",
      "\n",
      "Testing candidate pool size=75...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:19<00:00, 10.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pool size=75 results: {1: 0.575, 5: 0.6236666666666667, 10: 0.6236666666666667}\n",
      "\n",
      "Testing candidate pool size=100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:21<00:00,  9.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pool size=100 results: {1: 0.57, 5: 0.6201666666666666, 10: 0.6201666666666666}\n",
      "\n",
      "Best pool size: 75 with MRR@5: 0.6236666666666667\n",
      "\n",
      "=== Generating Final Predictions ===\n",
      "Using alpha=0.3, pool_size=75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [02:32<00:00,  9.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final predictions saved to 'optimized_predictions.tsv'\n",
      "Final prediction results: {1: 0.5814285714285714, 5: 0.6234047619047619, 10: 0.6234047619047619}\n",
      "\n",
      "Optimization complete! Best parameters: alpha=0.3, pool_size=75\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from rank_bm25 import BM25Okapi\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Load data first\n",
    "print(\"Loading data...\")\n",
    "# Update these paths as needed\n",
    "PATH_COLLECTION_DATA = 'subtask4b_collection_data.pkl'\n",
    "PATH_QUERY_TRAIN_DATA = 'subtask4b_query_tweets_train.tsv'\n",
    "PATH_QUERY_DEV_DATA = 'subtask4b_query_tweets_dev.tsv'\n",
    "\n",
    "df_collection = pd.read_pickle(PATH_COLLECTION_DATA)\n",
    "df_query_train = pd.read_csv(PATH_QUERY_TRAIN_DATA, sep='\\t')\n",
    "df_query_dev = pd.read_csv(PATH_QUERY_DEV_DATA, sep='\\t')\n",
    "\n",
    "# 2. Create BM25 index\n",
    "print(\"Creating BM25 index...\")\n",
    "corpus = df_collection[:][['title', 'abstract']].apply(lambda x: f\"{x['title']} {x['abstract']}\", axis=1).tolist()\n",
    "cord_uids = df_collection[:]['cord_uid'].tolist()\n",
    "tokenized_corpus = [doc.split(' ') for doc in corpus]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# 3. BM25 retrieval function\n",
    "def get_top_cord_uids_extended(query, k=20):\n",
    "    \"\"\"Get top-k candidates using BM25 with adjustable k\"\"\"\n",
    "    tokenized_query = query.split(' ')\n",
    "    doc_scores = bm25.get_scores(tokenized_query)\n",
    "    indices = np.argsort(-doc_scores)[:k]\n",
    "    bm25_topk = [cord_uids[x] for x in indices]\n",
    "    bm25_scores = [doc_scores[x] for x in indices]\n",
    "\n",
    "    return bm25_topk, bm25_scores\n",
    "\n",
    "# 4. Evaluation function\n",
    "def get_performance_mrr(data, col_gold, col_pred, list_k = [1, 5, 10]):\n",
    "    d_performance = {}\n",
    "    for k in list_k:\n",
    "        data[\"in_topx\"] = data.apply(lambda x: (1/([i for i in x[col_pred][:k]].index(x[col_gold]) + 1) if x[col_gold] in [i for i in x[col_pred][:k]] else 0), axis=1)\n",
    "        d_performance[k] = data[\"in_topx\"].mean()\n",
    "    return d_performance\n",
    "\n",
    "# 5. Define the optimized NeuralReranker\n",
    "class OptimizedReranker:\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.corpus_embeddings = None\n",
    "        self.corpus_texts = None\n",
    "        self.paper_ids = None\n",
    "        \n",
    "    def index_collection(self, df_collection):\n",
    "        # Create text representation for each document\n",
    "        self.corpus_texts = df_collection[:][['title', 'abstract']].apply(\n",
    "            lambda x: f\"{x['title']} {x['abstract']}\", axis=1).tolist()\n",
    "        self.paper_ids = df_collection[:]['cord_uid'].tolist()\n",
    "        \n",
    "        # Calculate embeddings for all documents\n",
    "        print(\"Calculating document embeddings...\")\n",
    "        self.corpus_embeddings = self.model.encode(\n",
    "            self.corpus_texts, \n",
    "            convert_to_tensor=True,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        print(f\"Created embeddings for {len(self.corpus_texts)} documents\")\n",
    "    \n",
    "    def rerank_candidates(self, query, candidate_ids, candidate_scores=None, top_k=5, alpha=0.3):\n",
    "        \"\"\"Re-rank the candidate documents with adjustable alpha parameter\"\"\"\n",
    "        # Get query embedding\n",
    "        query_embedding = self.model.encode(query, convert_to_tensor=True)\n",
    "        \n",
    "        # Get embeddings for candidate documents\n",
    "        candidate_indices = [self.paper_ids.index(cid) for cid in candidate_ids]\n",
    "        candidate_embeddings = self.corpus_embeddings[candidate_indices]\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        cos_scores = util.cos_sim(query_embedding, candidate_embeddings)[0]\n",
    "        \n",
    "        # Combine scores with adjustable alpha\n",
    "        if candidate_scores is not None:\n",
    "            bm25_scores = torch.tensor(candidate_scores)\n",
    "            bm25_scores = bm25_scores / bm25_scores.max()\n",
    "            \n",
    "            # Alpha controls weight between BM25 and neural scores\n",
    "            combined_scores = alpha * bm25_scores + (1-alpha) * cos_scores\n",
    "        else:\n",
    "            combined_scores = cos_scores\n",
    "            \n",
    "        # Sort by score\n",
    "        top_results = torch.argsort(-combined_scores)[:top_k].tolist()\n",
    "        \n",
    "        # Return re-ranked document IDs\n",
    "        return [candidate_ids[i] for i in top_results]\n",
    "\n",
    "# 6. Alpha parameter optimization function\n",
    "def evaluate_alpha_values(reranker, df_queries, alphas=[0.2, 0.3, 0.4, 0.5], top_k=5):\n",
    "    \"\"\"Test different alpha values for combining BM25 and neural scores\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        print(f\"Testing alpha={alpha}...\")\n",
    "        alpha_results = []\n",
    "        \n",
    "        for _, row in tqdm(df_queries.iterrows(), total=len(df_queries)):\n",
    "            query = row['tweet_text']\n",
    "            \n",
    "            # First-stage: Get BM25 candidates\n",
    "            bm25_candidates, bm25_scores = get_top_cord_uids_extended(query)\n",
    "            \n",
    "            # Second-stage: Neural re-ranking with current alpha\n",
    "            reranked_candidates = reranker.rerank_candidates(\n",
    "                query, \n",
    "                bm25_candidates, \n",
    "                bm25_scores, \n",
    "                top_k=top_k,\n",
    "                alpha=alpha\n",
    "            )\n",
    "            \n",
    "            alpha_results.append({\n",
    "                'post_id': row['post_id'],\n",
    "                'tweet_text': query,\n",
    "                'cord_uid': row['cord_uid'],\n",
    "                'reranked': reranked_candidates\n",
    "            })\n",
    "        \n",
    "        # Evaluate MRR scores\n",
    "        alpha_df = pd.DataFrame(alpha_results)\n",
    "        mrr_scores = get_performance_mrr(alpha_df, 'cord_uid', 'reranked')\n",
    "        results[alpha] = mrr_scores\n",
    "        print(f\"Alpha={alpha} results: {mrr_scores}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 7. BM25 candidate pool size optimization function\n",
    "def evaluate_candidate_pool_sizes(reranker, df_queries, pool_sizes=[20, 30, 50, 100], alpha=0.3, top_k=5):\n",
    "    \"\"\"Test different candidate pool sizes for BM25 retrieval\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for pool_size in pool_sizes:\n",
    "        print(f\"\\nTesting candidate pool size={pool_size}...\")\n",
    "        size_results = []\n",
    "        \n",
    "        for _, row in tqdm(df_queries.iterrows(), total=len(df_queries)):\n",
    "            query = row['tweet_text']\n",
    "            \n",
    "            # First-stage: Get BM25 candidates with current pool size\n",
    "            bm25_candidates, bm25_scores = get_top_cord_uids_extended(query, k=pool_size)\n",
    "            \n",
    "            # Second-stage: Neural re-ranking\n",
    "            reranked_candidates = reranker.rerank_candidates(\n",
    "                query, \n",
    "                bm25_candidates, \n",
    "                bm25_scores, \n",
    "                top_k=top_k,\n",
    "                alpha=alpha\n",
    "            )\n",
    "            \n",
    "            size_results.append({\n",
    "                'post_id': row['post_id'],\n",
    "                'tweet_text': query,\n",
    "                'cord_uid': row['cord_uid'],\n",
    "                'reranked': reranked_candidates\n",
    "            })\n",
    "        \n",
    "        # Evaluate MRR scores\n",
    "        size_df = pd.DataFrame(size_results)\n",
    "        mrr_scores = get_performance_mrr(size_df, 'cord_uid', 'reranked')\n",
    "        results[pool_size] = mrr_scores\n",
    "        print(f\"Pool size={pool_size} results: {mrr_scores}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 8. Complete optimization pipeline\n",
    "def run_optimization():\n",
    "    # Initialize and index with optimized reranker\n",
    "    print(\"Initializing optimized reranker...\")\n",
    "    optimized_reranker = OptimizedReranker('all-MiniLM-L6-v2')\n",
    "    optimized_reranker.index_collection(df_collection)\n",
    "    \n",
    "    # Use a subset of dev data for faster testing during optimization\n",
    "    dev_subset = df_query_dev.sample(min(200, len(df_query_dev)), random_state=42)\n",
    "    \n",
    "    # 1. Find optimal alpha\n",
    "    print(\"\\n=== Alpha Parameter Optimization ===\")\n",
    "    alpha_results = evaluate_alpha_values(\n",
    "        optimized_reranker,\n",
    "        dev_subset,\n",
    "        alphas=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "    )\n",
    "    best_alpha = max(alpha_results.items(), key=lambda x: x[1][5])[0]\n",
    "    print(f\"\\nBest alpha: {best_alpha} with MRR@5: {alpha_results[best_alpha][5]}\")\n",
    "    \n",
    "    # 2. Find optimal pool size using best alpha\n",
    "    print(\"\\n=== Pool Size Optimization ===\")\n",
    "    pool_results = evaluate_candidate_pool_sizes(\n",
    "        optimized_reranker,\n",
    "        dev_subset,\n",
    "        pool_sizes=[20, 30, 50, 75, 100],\n",
    "        alpha=best_alpha\n",
    "    )\n",
    "    best_pool_size = max(pool_results.items(), key=lambda x: x[1][5])[0]\n",
    "    print(f\"\\nBest pool size: {best_pool_size} with MRR@5: {pool_results[best_pool_size][5]}\")\n",
    "    \n",
    "    # 3. Generate final predictions with optimal parameters\n",
    "    print(\"\\n=== Generating Final Predictions ===\")\n",
    "    final_results = []\n",
    "    \n",
    "    print(f\"Using alpha={best_alpha}, pool_size={best_pool_size}\")\n",
    "    for _, row in tqdm(df_query_dev.iterrows(), total=len(df_query_dev)):\n",
    "        query = row['tweet_text']\n",
    "        \n",
    "        # First-stage: Get BM25 candidates with best pool size\n",
    "        bm25_candidates, bm25_scores = get_top_cord_uids_extended(query, k=best_pool_size)\n",
    "        \n",
    "        # Second-stage: Neural re-ranking with best alpha\n",
    "        reranked_candidates = optimized_reranker.rerank_candidates(\n",
    "            query, \n",
    "            bm25_candidates, \n",
    "            bm25_scores, \n",
    "            top_k=5,\n",
    "            alpha=best_alpha\n",
    "        )\n",
    "        \n",
    "        final_results.append({\n",
    "            'post_id': row['post_id'],\n",
    "            'preds': reranked_candidates\n",
    "        })\n",
    "    \n",
    "    # Save final predictions\n",
    "    final_df = pd.DataFrame(final_results)\n",
    "    final_df.to_csv('optimized_predictions.tsv', index=None, sep='\\t')\n",
    "    print(\"Final predictions saved to 'optimized_predictions.tsv'\")\n",
    "    \n",
    "    # Evaluate final predictions on the full dev set\n",
    "    final_eval_data = []\n",
    "    for _, row in df_query_dev.iterrows():\n",
    "        post_id = row['post_id']\n",
    "        gold_uid = row['cord_uid']\n",
    "        pred_row = final_df[final_df['post_id'] == post_id].iloc[0]\n",
    "        preds = pred_row['preds']\n",
    "        \n",
    "        final_eval_data.append({\n",
    "            'post_id': post_id,\n",
    "            'cord_uid': gold_uid,\n",
    "            'reranked': preds\n",
    "        })\n",
    "    \n",
    "    # Calculate MRR scores for final results\n",
    "    final_eval_df = pd.DataFrame(final_eval_data)\n",
    "    final_mrr_scores = get_performance_mrr(final_eval_df, 'cord_uid', 'reranked', list_k=[1, 5, 10])\n",
    "    print(f\"Final prediction results: {final_mrr_scores}\")\n",
    "    \n",
    "    return best_alpha, best_pool_size, final_df, final_mrr_scores\n",
    "\n",
    "# Run the optimization\n",
    "if __name__ == \"__main__\":\n",
    "    best_alpha, best_pool_size, predictions, final_results = run_optimization()\n",
    "    print(f\"\\nOptimization complete! Best parameters: alpha={best_alpha}, pool_size={best_pool_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Executing neural re-ranking for development phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing required packages...\n",
      "Packages installed\n",
      "NLTK is available with all required resources\n",
      "spaCy models not available. Entity extraction will be limited.\n",
      "Running enhanced scientific paper retrieval experiment...\n",
      "=== Enhanced Scientific Retrieval Experiment ===\n",
      "Creating enhanced BM25 index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jakob\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating enhanced document representations...\n",
      "Calculating document embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 242/242 [11:57<00:00,  2.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created embeddings for 7718 documents\n",
      "Extracting entities from documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7718/7718 [00:06<00:00, 1223.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Generating Enhanced Predictions ===\n",
      "Using alpha=0.3, pool_size=75, context_weight=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [11:05<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced predictions saved to 'enhanced_predictions.tsv'\n",
      "Enhanced prediction results: {1: 0.6035714285714285, 5: 0.649452380952381, 10: 0.649452380952381}\n",
      "\n",
      "Enhanced retrieval experiment complete!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "import string\n",
    "from datetime import datetime\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Try to install required packages\n",
    "try:\n",
    "    import pip\n",
    "    print(\"Installing required packages...\")\n",
    "    import sys\n",
    "    from subprocess import call\n",
    "    call([sys.executable, \"-m\", \"pip\", \"install\", \"nltk\", \"scikit-learn\"])\n",
    "    print(\"Packages installed\")\n",
    "except:\n",
    "    print(\"Could not install packages automatically. Some features may be limited.\")\n",
    "\n",
    "# Try imports with fallbacks\n",
    "try:\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "except ImportError:\n",
    "    CountVectorizer = None\n",
    "    print(\"scikit-learn not available. Some features will be limited.\")\n",
    "\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    # Attempt to download required NLTK resources\n",
    "    try:\n",
    "        nltk.download('punkt', quiet=True)\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "        from nltk.corpus import stopwords\n",
    "        from nltk.stem import PorterStemmer\n",
    "        NLTK_AVAILABLE = True\n",
    "        print(\"NLTK is available with all required resources\")\n",
    "    except:\n",
    "        NLTK_AVAILABLE = False\n",
    "        print(\"NLTK available but couldn't download resources\")\n",
    "except ImportError:\n",
    "    NLTK_AVAILABLE = False\n",
    "    print(\"NLTK not available. Will use basic text processing.\")\n",
    "\n",
    "# Try to import spaCy with fallback\n",
    "try:\n",
    "    import spacy\n",
    "    try:\n",
    "        nlp = spacy.load(\"en_core_sci_md\")  # Scientific/biomedical model\n",
    "        SPACY_AVAILABLE = True\n",
    "        SCIENTIFIC_MODEL = True\n",
    "        print(\"Loaded scientific spaCy model\")\n",
    "    except:\n",
    "        try:\n",
    "            nlp = spacy.load(\"en_core_web_sm\")  # Fallback to default model\n",
    "            SPACY_AVAILABLE = True\n",
    "            SCIENTIFIC_MODEL = False\n",
    "            print(\"Loaded default spaCy model\")\n",
    "        except:\n",
    "            SPACY_AVAILABLE = False\n",
    "            nlp = None\n",
    "            print(\"spaCy models not available. Entity extraction will be limited.\")\n",
    "except ImportError:\n",
    "    SPACY_AVAILABLE = False\n",
    "    nlp = None\n",
    "    print(\"spaCy not available. Entity extraction will be limited.\")\n",
    "\n",
    "# Dictionary of common scientific abbreviations\n",
    "SCI_ABBREVIATIONS = {\n",
    "    'covid': 'coronavirus disease',\n",
    "    'sars': 'severe acute respiratory syndrome',\n",
    "    'icu': 'intensive care unit',\n",
    "    'pcr': 'polymerase chain reaction',\n",
    "    'mrna': 'messenger rna',\n",
    "    'ace2': 'angiotensin-converting enzyme 2',\n",
    "    'rt-pcr': 'reverse transcription polymerase chain reaction',\n",
    "    'r0': 'basic reproduction number',\n",
    "    'rct': 'randomized controlled trial',\n",
    "    # Add more relevant scientific abbreviations as needed\n",
    "}\n",
    "\n",
    "# Basic stopwords list if NLTK is not available\n",
    "BASIC_STOPWORDS = set([\n",
    "    'a', 'an', 'the', 'and', 'or', 'but', 'if', 'because', 'as', 'what',\n",
    "    'which', 'this', 'that', 'these', 'those', 'then', 'just', 'so', 'than',\n",
    "    'such', 'both', 'through', 'about', 'for', 'is', 'of', 'while', 'during',\n",
    "    'to', 'from', 'in', 'on', 'by', 'at', 'with'\n",
    "])\n",
    "\n",
    "# Scientific stopwords to remove (words common in papers but not informative)\n",
    "SCIENTIFIC_STOPWORDS = set([\n",
    "    'study', 'studies', 'research', 'researchers', 'paper', 'figure', 'table',\n",
    "    'et', 'al', 'doi', 'pmid', 'journal', 'abstract', 'conclusion', 'results',\n",
    "    'methods', 'published', 'authors'\n",
    "])\n",
    "\n",
    "# Fallback tokenizer if NLTK is not available\n",
    "def simple_tokenize(text):\n",
    "    \"\"\"Simple tokenization function as fallback\"\"\"\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    return tokens\n",
    "\n",
    "# Get appropriate tokenize function\n",
    "if NLTK_AVAILABLE:\n",
    "    tokenize_func = word_tokenize\n",
    "else:\n",
    "    tokenize_func = simple_tokenize\n",
    "\n",
    "# Get appropriate stopwords\n",
    "if NLTK_AVAILABLE:\n",
    "    try:\n",
    "        STOPWORDS = set(stopwords.words('english')).union(SCIENTIFIC_STOPWORDS)\n",
    "    except:\n",
    "        STOPWORDS = BASIC_STOPWORDS.union(SCIENTIFIC_STOPWORDS)\n",
    "else:\n",
    "    STOPWORDS = BASIC_STOPWORDS.union(SCIENTIFIC_STOPWORDS)\n",
    "\n",
    "# Preprocess text with enhanced scientific processing\n",
    "def preprocess_scientific_text(text, expand_abbreviations=True, remove_punct=True):\n",
    "    \"\"\"Enhanced scientific text preprocessing with fallbacks for missing libraries\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Expand common scientific abbreviations\n",
    "    if expand_abbreviations:\n",
    "        for abbr, expansion in SCI_ABBREVIATIONS.items():\n",
    "            # Match whole word only (with word boundaries)\n",
    "            text = re.sub(r'\\b' + abbr + r'\\b', expansion, text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Handle hashtags - keep the text without # symbol\n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    \n",
    "    # Handle mentions - remove them\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    if remove_punct:\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Extract entities using spaCy, with fallback\n",
    "def extract_entities(text):\n",
    "    \"\"\"Extract scientific entities from text using spaCy if available, otherwise use simple approach\"\"\"\n",
    "    if SPACY_AVAILABLE and nlp is not None and text:\n",
    "        doc = nlp(text)\n",
    "        entities = []\n",
    "        \n",
    "        # Extract named entities\n",
    "        for ent in doc.ents:\n",
    "            entities.append(ent.text.lower())\n",
    "        \n",
    "        # For scientific text, also extract noun chunks as they often represent concepts\n",
    "        for chunk in doc.noun_chunks:\n",
    "            entities.append(chunk.text.lower())\n",
    "        \n",
    "        return list(set(entities))  # Remove duplicates\n",
    "    else:\n",
    "        # Fallback: just use n-grams as proxy for entities\n",
    "        words = preprocess_scientific_text(text).split()\n",
    "        # Create bigrams and trigrams as simple entity proxies\n",
    "        entities = words.copy()  # Start with unigrams\n",
    "        \n",
    "        # Add bigrams if we have enough words\n",
    "        if len(words) >= 2:\n",
    "            for i in range(len(words) - 1):\n",
    "                entities.append(f\"{words[i]} {words[i+1]}\")\n",
    "        \n",
    "        # Add trigrams if we have enough words\n",
    "        if len(words) >= 3:\n",
    "            for i in range(len(words) - 2):\n",
    "                entities.append(f\"{words[i]} {words[i+1]} {words[i+2]}\")\n",
    "                \n",
    "        return entities\n",
    "\n",
    "# Create enhanced document representation with weighted fields and metadata\n",
    "def create_enhanced_document_representation(row, title_weight=2.0, use_metadata=True):\n",
    "    \"\"\"\n",
    "    Create enhanced document representation with weighted title and metadata\n",
    "    Args:\n",
    "        row: DataFrame row with paper data\n",
    "        title_weight: Weight to apply to title (higher means more importance)\n",
    "        use_metadata: Whether to include authors and journal info\n",
    "    Returns:\n",
    "        Enhanced document representation string\n",
    "    \"\"\"\n",
    "    # Basic representation with weighted title (repeat title for higher weight)\n",
    "    components = []\n",
    "    \n",
    "    # Add weighted title\n",
    "    if 'title' in row and pd.notna(row['title']):\n",
    "        components.extend([row['title']] * int(title_weight))\n",
    "    \n",
    "    # Add abstract\n",
    "    if 'abstract' in row and pd.notna(row['abstract']):\n",
    "        components.append(row['abstract'])\n",
    "    \n",
    "    # Add metadata if requested\n",
    "    if use_metadata:\n",
    "        # Add authors (if available)\n",
    "        if 'authors' in row and pd.notna(row['authors']):\n",
    "            components.append(row['authors'])\n",
    "        \n",
    "        # Add journal (if available)\n",
    "        if 'journal' in row and pd.notna(row['journal']):\n",
    "            components.append(row['journal'])\n",
    "    \n",
    "    # Join all components\n",
    "    text = \" \".join(components)\n",
    "    \n",
    "    # Preprocess\n",
    "    return preprocess_scientific_text(text)\n",
    "\n",
    "# Class for advanced scientific paper retrieval\n",
    "class EnhancedScientificRetriever:\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.corpus_embeddings = None\n",
    "        self.corpus_texts = None\n",
    "        self.paper_ids = None\n",
    "        self.metadata = None\n",
    "        \n",
    "    def index_collection(self, df_collection, title_weight=2.0, use_metadata=True):\n",
    "        \"\"\"\n",
    "        Index the collection with enhanced document representation\n",
    "        Args:\n",
    "            df_collection: DataFrame with papers\n",
    "            title_weight: Weight for the title\n",
    "            use_metadata: Whether to include metadata\n",
    "        \"\"\"\n",
    "        # Store paper metadata for later use in features\n",
    "        self.metadata = {}\n",
    "        for _, row in df_collection.iterrows():\n",
    "            paper_id = row['cord_uid']\n",
    "            self.metadata[paper_id] = {\n",
    "                'time': row['time'] if pd.notna(row['time']) else None,\n",
    "                'journal': row['journal'] if pd.notna(row['journal']) else \"\",\n",
    "                'authors': row['authors'] if pd.notna(row['authors']) else \"\",\n",
    "                'title': row['title'] if pd.notna(row['title']) else \"\",\n",
    "                'abstract': row['abstract'] if pd.notna(row['abstract']) else \"\"\n",
    "            }\n",
    "        \n",
    "        # Create enhanced text representation for each document\n",
    "        self.corpus_texts = []\n",
    "        self.paper_ids = []\n",
    "        \n",
    "        print(\"Creating enhanced document representations...\")\n",
    "        for _, row in df_collection.iterrows():\n",
    "            enhanced_text = create_enhanced_document_representation(\n",
    "                row, title_weight=title_weight, use_metadata=use_metadata\n",
    "            )\n",
    "            self.corpus_texts.append(enhanced_text)\n",
    "            self.paper_ids.append(row['cord_uid'])\n",
    "        \n",
    "        # Calculate embeddings for all documents\n",
    "        print(\"Calculating document embeddings...\")\n",
    "        self.corpus_embeddings = self.model.encode(\n",
    "            self.corpus_texts, \n",
    "            convert_to_tensor=True,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        print(f\"Created embeddings for {len(self.corpus_texts)} documents\")\n",
    "        \n",
    "        # Extract and store entities from documents for later matching\n",
    "        print(\"Extracting entities from documents...\")\n",
    "        self.doc_entities = {}\n",
    "        for i, doc_id in enumerate(tqdm(self.paper_ids)):\n",
    "            text = self.corpus_texts[i]\n",
    "            self.doc_entities[doc_id] = extract_entities(text)\n",
    "    \n",
    "    def preprocess_tweet(self, tweet_text):\n",
    "        \"\"\"Enhanced tweet preprocessing\"\"\"\n",
    "        # Apply scientific text preprocessing\n",
    "        processed_tweet = preprocess_scientific_text(tweet_text)\n",
    "        return processed_tweet\n",
    "    \n",
    "    def calculate_recency_score(self, paper_id, tweet_date=None):\n",
    "        \"\"\"\n",
    "        Calculate recency score to favor more recent papers\n",
    "        Args:\n",
    "            paper_id: The paper ID\n",
    "            tweet_date: Date of the tweet (if known)\n",
    "        Returns:\n",
    "            Recency score between 0 and 1 (higher for more recent papers)\n",
    "        \"\"\"\n",
    "        if paper_id not in self.metadata or self.metadata[paper_id]['time'] is None:\n",
    "            return 0.5  # Default score if no date\n",
    "        \n",
    "        paper_date = self.metadata[paper_id]['time']\n",
    "        \n",
    "        # If no tweet date, use a recent date\n",
    "        if tweet_date is None:\n",
    "            tweet_date = datetime(2020, 12, 31)  # End of 2020 as reference\n",
    "        \n",
    "        # Convert to timestamp if needed\n",
    "        if isinstance(paper_date, str):\n",
    "            try:\n",
    "                paper_date = datetime.strptime(paper_date, '%Y-%m-%d')\n",
    "            except:\n",
    "                return 0.5\n",
    "        \n",
    "        # Calculate days difference\n",
    "        days_diff = abs((tweet_date - paper_date).days)\n",
    "        \n",
    "        # Convert to score (1 for very recent, approaching 0 for very old)\n",
    "        # Using sigmoid function for smooth transition\n",
    "        recency_score = 1 / (1 + np.exp(days_diff / 365))  # 365 controls steepness\n",
    "        \n",
    "        return recency_score\n",
    "    \n",
    "    def calculate_relevance_bonus(self, query_entities, paper_id):\n",
    "        \"\"\"\n",
    "        Calculate bonus based on entity overlap between query and document\n",
    "        Args:\n",
    "            query_entities: List of entities in the query\n",
    "            paper_id: The paper ID\n",
    "        Returns:\n",
    "            Relevance bonus score between 0 and 1\n",
    "        \"\"\"\n",
    "        if not query_entities or paper_id not in self.doc_entities:\n",
    "            return 0\n",
    "            \n",
    "        doc_entities = self.doc_entities[paper_id]\n",
    "        \n",
    "        # Count overlapping entities\n",
    "        overlap = sum(1 for e in query_entities if any(e in d for d in doc_entities))\n",
    "        \n",
    "        # Calculate score based on overlap ratio\n",
    "        if len(query_entities) > 0:\n",
    "            return min(1.0, overlap / len(query_entities))\n",
    "        return 0\n",
    "    \n",
    "    def calculate_journal_score(self, paper_id):\n",
    "        \"\"\"\n",
    "        Calculate journal importance score (proxy for citation impact)\n",
    "        Args:\n",
    "            paper_id: The paper ID\n",
    "        Returns:\n",
    "            Journal score between 0 and 1\n",
    "        \"\"\"\n",
    "        if paper_id not in self.metadata:\n",
    "            return 0.5\n",
    "            \n",
    "        journal = self.metadata[paper_id]['journal']\n",
    "        \n",
    "        # List of high-impact journals (customize based on your domain)\n",
    "        high_impact_journals = {\n",
    "            'nature': 1.0,\n",
    "            'science': 1.0,\n",
    "            'lancet': 0.9,\n",
    "            'nejm': 0.9,\n",
    "            'new england journal': 0.9,\n",
    "            'jama': 0.8,\n",
    "            'bmj': 0.8,\n",
    "            'cell': 0.8,\n",
    "            'pnas': 0.7,\n",
    "            'plos': 0.6\n",
    "        }\n",
    "        \n",
    "        # Check for journal match\n",
    "        if journal:\n",
    "            journal_lower = journal.lower()\n",
    "            for j_name, score in high_impact_journals.items():\n",
    "                if j_name in journal_lower:\n",
    "                    return score\n",
    "        \n",
    "        # Default score\n",
    "        return 0.5\n",
    "    \n",
    "    def rerank_candidates(self, query, candidate_ids, bm25_scores=None, top_k=5, \n",
    "                          alpha=0.3, use_context_features=True, context_weight=0.2):\n",
    "        \"\"\"\n",
    "        Re-rank candidate documents with enhanced features\n",
    "        Args:\n",
    "            query: The tweet text\n",
    "            candidate_ids: List of candidate paper IDs\n",
    "            bm25_scores: BM25 scores for candidates\n",
    "            top_k: Number of top results to return\n",
    "            alpha: Weight between BM25 and neural scores\n",
    "            use_context_features: Whether to use context-aware features\n",
    "            context_weight: Weight for context features\n",
    "        Returns:\n",
    "            List of reranked document IDs\n",
    "        \"\"\"\n",
    "        # Preprocess the query\n",
    "        processed_query = self.preprocess_tweet(query)\n",
    "        \n",
    "        # Get query embedding\n",
    "        query_embedding = self.model.encode(processed_query, convert_to_tensor=True)\n",
    "        \n",
    "        # Get embeddings for candidate documents\n",
    "        candidate_indices = [self.paper_ids.index(cid) for cid in candidate_ids]\n",
    "        candidate_embeddings = self.corpus_embeddings[candidate_indices]\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        cos_scores = util.cos_sim(query_embedding, candidate_embeddings)[0]\n",
    "        \n",
    "        # Combine BM25 and neural scores\n",
    "        if bm25_scores is not None:\n",
    "            bm25_tensor = torch.tensor(bm25_scores)\n",
    "            bm25_norm = bm25_tensor / bm25_tensor.max() if torch.max(bm25_tensor) > 0 else bm25_tensor\n",
    "            combined_scores = alpha * bm25_norm + (1-alpha) * cos_scores\n",
    "        else:\n",
    "            combined_scores = cos_scores\n",
    "        \n",
    "        # Apply context-aware features if requested\n",
    "        if use_context_features:\n",
    "            # Extract entities from query\n",
    "            query_entities = extract_entities(processed_query)\n",
    "            \n",
    "            # Calculate context scores for each candidate\n",
    "            context_scores = []\n",
    "            for i, paper_id in enumerate(candidate_ids):\n",
    "                # Recency score\n",
    "                recency_score = self.calculate_recency_score(paper_id)\n",
    "                \n",
    "                # Entity relevance score\n",
    "                relevance_score = self.calculate_relevance_bonus(query_entities, paper_id)\n",
    "                \n",
    "                # Journal importance score\n",
    "                journal_score = self.calculate_journal_score(paper_id)\n",
    "                \n",
    "                # Combine context scores (adjust weights as needed)\n",
    "                context_score = 0.4 * recency_score + 0.4 * relevance_score + 0.2 * journal_score\n",
    "                context_scores.append(context_score)\n",
    "            \n",
    "            # Convert to tensor and combine with other scores\n",
    "            context_tensor = torch.tensor(context_scores)\n",
    "            final_scores = (1 - context_weight) * combined_scores + context_weight * context_tensor\n",
    "        else:\n",
    "            final_scores = combined_scores\n",
    "        \n",
    "        # Sort by score\n",
    "        top_results = torch.argsort(-final_scores)[:top_k].tolist()\n",
    "        \n",
    "        # Return re-ranked document IDs\n",
    "        return [candidate_ids[i] for i in top_results]\n",
    "\n",
    "# Create enhanced BM25 index with improved preprocessing\n",
    "def create_enhanced_bm25_index(df_collection, title_weight=2.0):\n",
    "    \"\"\"\n",
    "    Create an enhanced BM25 index with better document representation\n",
    "    Args:\n",
    "        df_collection: DataFrame with papers\n",
    "        title_weight: Weight for title\n",
    "    Returns:\n",
    "        BM25 index and paper IDs\n",
    "    \"\"\"\n",
    "    print(\"Creating enhanced BM25 index...\")\n",
    "    \n",
    "    # Create enhanced document representations\n",
    "    corpus = []\n",
    "    cord_uids = []\n",
    "    \n",
    "    for _, row in df_collection.iterrows():\n",
    "        # Create enhanced text\n",
    "        enhanced_text = create_enhanced_document_representation(\n",
    "            row, title_weight=title_weight, use_metadata=True\n",
    "        )\n",
    "        corpus.append(enhanced_text)\n",
    "        cord_uids.append(row['cord_uid'])\n",
    "    \n",
    "    # Tokenize\n",
    "    tokenized_corpus = [doc.split() for doc in corpus]\n",
    "    \n",
    "    # Create BM25 index\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    \n",
    "    return bm25, cord_uids\n",
    "\n",
    "# Enhanced BM25 retrieval with improved query processing\n",
    "def get_enhanced_top_cord_uids(query, bm25, cord_uids, k=75):\n",
    "    \"\"\"\n",
    "    Get top-k candidates using enhanced BM25\n",
    "    Args:\n",
    "        query: Tweet text\n",
    "        bm25: BM25 index\n",
    "        cord_uids: List of paper IDs\n",
    "        k: Number of results to return\n",
    "    Returns:\n",
    "        List of paper IDs and scores\n",
    "    \"\"\"\n",
    "    # Preprocess query\n",
    "    processed_query = preprocess_scientific_text(query)\n",
    "    tokenized_query = processed_query.split()\n",
    "    \n",
    "    # Get scores\n",
    "    doc_scores = bm25.get_scores(tokenized_query)\n",
    "    \n",
    "    # Sort and return top k\n",
    "    indices = np.argsort(-doc_scores)[:k]\n",
    "    bm25_topk = [cord_uids[x] for x in indices]\n",
    "    bm25_scores = [doc_scores[x] for x in indices]\n",
    "    \n",
    "    return bm25_topk, bm25_scores\n",
    "\n",
    "# Implementation of the whole optimization pipeline\n",
    "def run_enhanced_retrieval_experiment(df_collection, df_train, df_test, \n",
    "                                     title_weight=2.0, alpha=0.3, pool_size=75,\n",
    "                                     use_context=True, context_weight=0.2,\n",
    "                                     top_k=5, model_name='all-MiniLM-L6-v2'):\n",
    "    \"\"\"\n",
    "    Run a complete retrieval experiment with all enhancements\n",
    "    Args:\n",
    "        df_collection: DataFrame with papers\n",
    "        df_train: Training queries DataFrame\n",
    "        df_test: Testing queries DataFrame\n",
    "        title_weight: Weight for title in document representation\n",
    "        alpha: Weight between BM25 and neural scores\n",
    "        pool_size: Size of candidate pool from BM25\n",
    "        use_context: Whether to use context-aware features\n",
    "        context_weight: Weight for context features\n",
    "        top_k: Number of results to return\n",
    "        model_name: Name of the sentence transformer model to use\n",
    "    \"\"\"\n",
    "    print(\"=== Enhanced Scientific Retrieval Experiment ===\")\n",
    "    \n",
    "    # 1. Create enhanced BM25 index\n",
    "    enhanced_bm25, enhanced_cord_uids = create_enhanced_bm25_index(df_collection, title_weight)\n",
    "    \n",
    "    # 2. Initialize and index with enhanced retriever\n",
    "    # Check if the model is available - fallback to a simpler one if not\n",
    "    try:\n",
    "        enhanced_retriever = EnhancedScientificRetriever(model_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model {model_name}: {e}\")\n",
    "        print(\"Falling back to default model\")\n",
    "        enhanced_retriever = EnhancedScientificRetriever('all-MiniLM-L6-v2')\n",
    "    \n",
    "    enhanced_retriever.index_collection(df_collection, title_weight)\n",
    "    \n",
    "    # 3. Generate predictions\n",
    "    print(\"\\n=== Generating Enhanced Predictions ===\")\n",
    "    print(f\"Using alpha={alpha}, pool_size={pool_size}, context_weight={context_weight}\")\n",
    "    \n",
    "    enhanced_results = []\n",
    "    \n",
    "    for _, row in tqdm(df_test.iterrows(), total=len(df_test)):\n",
    "        query = row['tweet_text']\n",
    "        \n",
    "        # First-stage: Get enhanced BM25 candidates\n",
    "        bm25_candidates, bm25_scores = get_enhanced_top_cord_uids(\n",
    "            query, enhanced_bm25, enhanced_cord_uids, k=pool_size\n",
    "        )\n",
    "        \n",
    "        # Second-stage: Enhanced neural re-ranking\n",
    "        reranked_candidates = enhanced_retriever.rerank_candidates(\n",
    "            query, \n",
    "            bm25_candidates, \n",
    "            bm25_scores, \n",
    "            top_k=top_k,\n",
    "            alpha=alpha,\n",
    "            use_context_features=use_context,\n",
    "            context_weight=context_weight\n",
    "        )\n",
    "        \n",
    "        enhanced_results.append({\n",
    "            'post_id': row['post_id'],\n",
    "            'preds': reranked_candidates\n",
    "        })\n",
    "    \n",
    "    # Save enhanced predictions\n",
    "    enhanced_df = pd.DataFrame(enhanced_results)\n",
    "    enhanced_df.to_csv('enhanced_predictions.tsv', index=None, sep='\\t')\n",
    "    print(\"Enhanced predictions saved to 'enhanced_predictions.tsv'\")\n",
    "    \n",
    "    # Evaluate enhanced predictions\n",
    "    enhanced_eval_data = []\n",
    "    for _, row in df_test.iterrows():\n",
    "        post_id = row['post_id']\n",
    "        gold_uid = row['cord_uid']\n",
    "        pred_row = enhanced_df[enhanced_df['post_id'] == post_id].iloc[0]\n",
    "        preds = pred_row['preds']\n",
    "        \n",
    "        enhanced_eval_data.append({\n",
    "            'post_id': post_id,\n",
    "            'cord_uid': gold_uid,\n",
    "            'reranked': preds\n",
    "        })\n",
    "    \n",
    "    # Calculate MRR scores using the same function as in original code\n",
    "    def get_performance_mrr(data, col_gold, col_pred, list_k = [1, 5, 10]):\n",
    "        d_performance = {}\n",
    "        for k in list_k:\n",
    "            data[\"in_topx\"] = data.apply(lambda x: (1/([i for i in x[col_pred][:k]].index(x[col_gold]) + 1) if x[col_gold] in [i for i in x[col_pred][:k]] else 0), axis=1)\n",
    "            d_performance[k] = data[\"in_topx\"].mean()\n",
    "        return d_performance\n",
    "    \n",
    "    enhanced_eval_df = pd.DataFrame(enhanced_eval_data)\n",
    "    enhanced_mrr_scores = get_performance_mrr(enhanced_eval_df, 'cord_uid', 'reranked', list_k=[1, 5, 10])\n",
    "    print(f\"Enhanced prediction results: {enhanced_mrr_scores}\")\n",
    "    \n",
    "    return enhanced_df, enhanced_mrr_scores\n",
    "\n",
    "# Add a simple main method with safe defaults - can be called directly\n",
    "try:\n",
    "    # Run the enhanced experiment with safer default choices\n",
    "    if __name__ == \"__main__\":\n",
    "        print(\"Running enhanced scientific paper retrieval experiment...\")\n",
    "        enhanced_predictions, enhanced_results = run_enhanced_retrieval_experiment(\n",
    "            df_collection, \n",
    "            df_query_train, \n",
    "            df_query_dev,\n",
    "            title_weight=3.0,      # Higher weight for title\n",
    "            alpha=0.3,             # Value from previous optimization\n",
    "            pool_size=75,          # Value from previous optimization\n",
    "            use_context=True,      # Enable context-aware features\n",
    "            context_weight=0.2,    # Weight for context features\n",
    "            top_k=5,               # Return top 5 results\n",
    "            model_name='all-MiniLM-L6-v2'  # Stick with the original model for safety\n",
    "        )\n",
    "        print(f\"\\nEnhanced retrieval experiment complete!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error running experiment: {e}\")\n",
    "    print(\"Please check error messages above and modify the code as needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Executing neural re-ranking for test phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Generating Test Set Predictions ===\n",
      "Using alpha=0.3, pool_size=75, context_weight=0.2\n",
      "Creating enhanced BM25 index...\n",
      "Creating enhanced BM25 index...\n",
      "Initializing enhanced retriever...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jakob\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating enhanced document representations...\n",
      "Calculating document embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 242/242 [12:26<00:00,  3.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created embeddings for 7718 documents\n",
      "Extracting entities from documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7718/7718 [00:06<00:00, 1225.99it/s]\n",
      "100%|██████████| 1446/1446 [09:46<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions saved to 'predictions.tsv'\n",
      "Test set processing complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the test data\n",
    "PATH_QUERY_TEST_DATA = 'subtask4b_query_tweets_test.tsv'\n",
    "df_query_test = pd.read_csv(PATH_QUERY_TEST_DATA, sep='\\t')\n",
    "\n",
    "# This function will process the test set using the already trained model\n",
    "def process_test_set(df_collection, df_test, \n",
    "                     enhanced_retriever=None, enhanced_bm25=None, enhanced_cord_uids=None,\n",
    "                     alpha=0.3, pool_size=75, context_weight=0.2, top_k=5):\n",
    "    \"\"\"\n",
    "    Generate predictions for the test set using the enhanced retrieval model\n",
    "    \"\"\"\n",
    "    print(\"=== Generating Test Set Predictions ===\")\n",
    "    print(f\"Using alpha={alpha}, pool_size={pool_size}, context_weight={context_weight}\")\n",
    "    \n",
    "    # If the models weren't passed in, we need to recreate them\n",
    "    if enhanced_retriever is None or enhanced_bm25 is None or enhanced_cord_uids is None:\n",
    "        print(\"Creating enhanced BM25 index...\")\n",
    "        enhanced_bm25, enhanced_cord_uids = create_enhanced_bm25_index(df_collection, title_weight=3.0)\n",
    "        \n",
    "        print(\"Initializing enhanced retriever...\")\n",
    "        enhanced_retriever = EnhancedScientificRetriever('all-MiniLM-L6-v2')\n",
    "        enhanced_retriever.index_collection(df_collection, title_weight=3.0)\n",
    "    \n",
    "    # Generate predictions\n",
    "    test_results = []\n",
    "    \n",
    "    for _, row in tqdm(df_test.iterrows(), total=len(df_test)):\n",
    "        query = row['tweet_text']\n",
    "        post_id = row['post_id']\n",
    "        \n",
    "        # First-stage: Get enhanced BM25 candidates\n",
    "        bm25_candidates, bm25_scores = get_enhanced_top_cord_uids(\n",
    "            query, enhanced_bm25, enhanced_cord_uids, k=pool_size\n",
    "        )\n",
    "        \n",
    "        # Second-stage: Enhanced neural re-ranking\n",
    "        reranked_candidates = enhanced_retriever.rerank_candidates(\n",
    "            query, \n",
    "            bm25_candidates, \n",
    "            bm25_scores, \n",
    "            top_k=top_k,\n",
    "            alpha=alpha,\n",
    "            use_context_features=True,\n",
    "            context_weight=context_weight\n",
    "        )\n",
    "        \n",
    "        test_results.append({\n",
    "            'post_id': post_id,\n",
    "            'preds': reranked_candidates\n",
    "        })\n",
    "    \n",
    "    # Save predictions\n",
    "    test_df = pd.DataFrame(test_results)\n",
    "    test_df.to_csv('predictions.tsv', index=None, sep='\\t')\n",
    "    print(\"Test predictions saved to 'predictions.tsv'\")\n",
    "    \n",
    "    return test_df\n",
    "\n",
    "# Process the test set - you can use the existing models if they're in memory,\n",
    "# otherwise they will be recreated\n",
    "test_predictions = process_test_set(\n",
    "    df_collection, \n",
    "    df_query_test,\n",
    "    enhanced_retriever=enhanced_retriever if 'enhanced_retriever' in locals() else None,\n",
    "    enhanced_bm25=enhanced_bm25 if 'enhanced_bm25' in locals() else None,\n",
    "    enhanced_cord_uids=enhanced_cord_uids if 'enhanced_cord_uids' in locals() else None,\n",
    "    alpha=0.3,            # Best value from optimization\n",
    "    pool_size=75,         # Best value from optimization\n",
    "    context_weight=0.2,   # Value used in enhanced experiment\n",
    "    top_k=5               # Return top 5 results\n",
    ")\n",
    "\n",
    "print(\"Test set processing complete!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
